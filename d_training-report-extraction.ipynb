{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2c5c2d",
   "metadata": {},
   "source": [
    "# Module 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75617e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Directories created and paths defined.\n",
      " Trained spaCy model loaded successfully from models/model-best.\n",
      "Found 9 corrected files to process.\n",
      "\n",
      "Successfully processed 9 documents.\n",
      "\n",
      "✅ Training data prepared and saved for inspection at: project_data\\spacy_training\\spacy_training_data.json\n",
      "\n",
      "Next step is to use this data to train a spaCy model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Assuming this script is in the same root directory as your project_data folder\n",
    "# Import directory paths from your main processor\n",
    "from e_inference import TOKEN_DIR, CORRECTIONS_DIR, DATA_DIR\n",
    "\n",
    "# Define where to save the final training data\n",
    "SPACY_TRAINING_DIR = os.path.join(DATA_DIR, 'spacy_training')\n",
    "os.makedirs(SPACY_TRAINING_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def find_token_sequence(tokens, target_text):\n",
    "    \"\"\"\n",
    "    Searches for a sequence of tokens that perfectly matches the target text.\n",
    "    Handles cases where the target text is split across multiple tokens.\n",
    "    \n",
    "    Returns:\n",
    "        A list of indices of the matching tokens, or None if not found.\n",
    "    \"\"\"\n",
    "    target_words = target_text.strip().split()\n",
    "    if not target_words:\n",
    "        return None\n",
    "\n",
    "    all_token_texts = [tok['text'] for tok in tokens]\n",
    "    \n",
    "    for i in range(len(all_token_texts) - len(target_words) + 1):\n",
    "        # Create a window of tokens to check\n",
    "        window = all_token_texts[i : i + len(target_words)]\n",
    "        \n",
    "        # Check if the sequence of tokens in the window matches the target words\n",
    "        if window == target_words:\n",
    "            return list(range(i, i + len(target_words))) # Return indices\n",
    "            \n",
    "    return None\n",
    "\n",
    "\n",
    "def create_training_data(limit=None):\n",
    "    \"\"\"\n",
    "    Processes corrected JSON files and original tokens to create a labeled dataset\n",
    "    in the BIO format, then converts it to spaCy's DocBin format. Can set limit to 80%\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    \n",
    "    # Get a list of corrected files(ground truth)\n",
    "    corrected_files = [f for f in os.listdir(CORRECTIONS_DIR) if f.endswith('_corrected.json')]\n",
    "    if limit:\n",
    "        corrected_files = corrected_files[:limit]\n",
    "        \n",
    "    print(f\"Found {len(corrected_files)} corrected files to process.\")\n",
    "\n",
    "    for filename in corrected_files:\n",
    "        # Construct paths for corrected data and original tokens\n",
    "        base_name = filename.replace('_corrected.json', '')\n",
    "        corrected_json_path = os.path.join(CORRECTIONS_DIR, filename)\n",
    "        token_json_path = os.path.join(TOKEN_DIR, f\"{base_name}_tokens.json\")\n",
    "\n",
    "        if not os.path.exists(token_json_path):\n",
    "            print(f\"WARNING: Token file not found for {filename}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(corrected_json_path, 'r', encoding='utf-8') as f:\n",
    "            corrected_data = json.load(f)\n",
    "        with open(token_json_path, 'r', encoding='utf-8') as f:\n",
    "            tokens = json.load(f)\n",
    "\n",
    "        # Initialize labels for all tokens as 'O' (Outside)\n",
    "        token_labels = ['O'] * len(tokens)\n",
    "\n",
    "        # --- 1. Tag Patient Info ---\n",
    "        for key, value in corrected_data.get('patient_info', {}).items():\n",
    "            # Create a simple, robust label from the key\n",
    "            label = key.upper().replace(' ', '_').replace('/', '_')\n",
    "            \n",
    "            indices = find_token_sequence(tokens, value)\n",
    "            if indices:\n",
    "                # Apply BIO tagging\n",
    "                token_labels[indices[0]] = f\"B-{label}\"\n",
    "                for i in indices[1:]:\n",
    "                    token_labels[i] = f\"I-{label}\"\n",
    "\n",
    "        # --- 2. Tag Lab Results ---\n",
    "        for result in corrected_data.get('lab_results', []):\n",
    "            for key, cell_data in result.items():\n",
    "                # The value might be a simple string or a dict {'value': '...', 'confidence': ...}\n",
    "                value = cell_data if isinstance(cell_data, str) else cell_data.get('value', '')\n",
    "                if not value:\n",
    "                    continue\n",
    "                \n",
    "                label = key.upper().replace(' ', '_').replace('/', '_')\n",
    "                \n",
    "                indices = find_token_sequence(tokens, value)\n",
    "                if indices:\n",
    "                    token_labels[indices[0]] = f\"B-{label}\"\n",
    "                    for i in indices[1:]:\n",
    "                        token_labels[i] = f\"I-{label}\"\n",
    "\n",
    "        # --- 3. Convert labeled tokens to spaCy's training format ---\n",
    "        # We'll treat the entire document as one large text block for simplicity\n",
    "        full_text = \" \".join([tok['text'] for tok in tokens])\n",
    "        entities = []\n",
    "        \n",
    "        current_char_pos = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            label = token_labels[i]\n",
    "            if label != 'O':\n",
    "                start_char = current_char_pos\n",
    "                end_char = start_char + len(token['text'])\n",
    "                # The BIO prefix (B- or I-) is only for our internal logic.\n",
    "                # spaCy just needs the core label (e.g., \"PATIENT_NAME\").\n",
    "                clean_label = label[2:] \n",
    "                entities.append((start_char, end_char, clean_label))\n",
    "            \n",
    "            # Update position for the next token (text + one space)\n",
    "            current_char_pos += len(token['text']) + 1\n",
    "        \n",
    "        if entities:\n",
    "            training_data.append((full_text, {\"entities\": entities}))\n",
    "\n",
    "    print(f\"\\nSuccessfully processed {len(training_data)} documents.\")\n",
    "    return training_data\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Generate the training data in spaCy's format\n",
    "    spacy_formatted_data = create_training_data()\n",
    "\n",
    "    # Save the data to a JSON file for inspection (optional, but recommended)\n",
    "    output_json_path = os.path.join(SPACY_TRAINING_DIR, 'spacy_training_data.json')\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(spacy_formatted_data, f, indent=4)\n",
    "        \n",
    "    print(f\"\\n✅ Training data prepared and saved for inspection at: {output_json_path}\")\n",
    "    print(\"\\nNext step is to use this data to train a spaCy model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed1133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ To generate a more effective transformer-based config (GPU-only),\n",
      "install the spacy-transformers package and re-run this command. The config\n",
      "generated now does not use transformers.\u001b[0m\n",
      "\u001b[38;5;4mℹ Generated config template specific for your use case\u001b[0m\n",
      "- Language: en\n",
      "- Pipeline: ner\n",
      "- Optimize for: efficiency\n",
      "- Hardware: CPU\n",
      "- Transformer: None\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# ! python -m spacy init config config.cfg --lang en --pipeline ner --optimize efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb485ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: 7 training examples, 2 development examples.\n",
      " Created train.spacy file at project_data\\spacy_training\n",
      "✅ Created dev.spacy file at project_data\\spacy_training\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import the path to the prepared data from the previous script\n",
    "# from prepare_training_data import SPACY_TRAINING_DIR\n",
    "\n",
    "def convert_to_spacy_format():\n",
    "    \"\"\"\n",
    "    Loads the JSON-formatted training data, splits it into training and validation sets,\n",
    "    and saves them in spaCy's .spacy binary format.\n",
    "    \"\"\"\n",
    "    input_json_path = Path(SPACY_TRAINING_DIR) / 'spacy_training_data.json'\n",
    "    output_dir = Path(SPACY_TRAINING_DIR)\n",
    "    \n",
    "    # Load the prepared data\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Split data into training (80%) and development (20%) sets\n",
    "    train_data, dev_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Data split: {len(train_data)} training examples, {len(dev_data)} development examples.\")\n",
    "\n",
    "    # Create a blank English model for tokenization\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    # Process and save the training data\n",
    "    db_train = DocBin()\n",
    "    for text, annotations in train_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annotations.get(\"entities\"):\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                print(f\"Skipping entity: Span couldn't be formed for '{text[start:end]}' in '{text}'\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db_train.add(doc)\n",
    "    db_train.to_disk(output_dir / \"train.spacy\")\n",
    "    print(f\" Created train.spacy file at {output_dir}\")\n",
    "\n",
    "    # Process and save the development (validation) data\n",
    "    db_dev = DocBin()\n",
    "    for text, annotations in dev_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annotations.get(\"entities\"):\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is None:\n",
    "                print(f\"Skipping entity: Span couldn't be formed for '{text[start:end]}' in '{text}'\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db_dev.add(doc)\n",
    "    db_dev.to_disk(output_dir / \"dev.spacy\")\n",
    "    print(f\"✅ Created dev.spacy file at {output_dir}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    convert_to_spacy_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c8f09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: models\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00    254.87    0.00    0.00    0.00    0.00\n",
      " 28     200       4429.25   7441.21   54.84   85.00   40.48    0.55\n",
      " 57     400        109.73    146.33   60.32   90.48   45.24    0.60\n",
      " 85     600         19.81     12.03   62.02   88.89   47.62    0.62\n",
      "114     800         48.07     13.07   62.02   88.89   47.62    0.62\n",
      "142    1000         27.96      7.07   60.00   84.78   46.43    0.60\n",
      "171    1200        181.06     53.97   57.14   85.71   42.86    0.57\n",
      "200    1400        228.70     48.79   50.79   76.19   38.10    0.51\n",
      "228    1600        337.26     55.99   55.81   80.00   42.86    0.56\n",
      "257    1800          8.22      2.18   62.02   88.89   47.62    0.62\n",
      "285    2000         29.14      5.90   59.38   86.36   45.24    0.59\n",
      "314    2200         22.26      2.46   54.69   79.55   41.67    0.55\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "models\\model-last\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy train config.cfg --output ./models --paths.train ./project_data/spacy_training/train.spacy --paths.dev ./project_data/spacy_training/dev.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93629458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
